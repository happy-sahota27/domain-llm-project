{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f69b7c",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import required libraries and initialize components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c25f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data.dataset_builder import DomainDatasetBuilder\n",
    "from src.data.preprocessing import DataPreprocessor\n",
    "from src.data.validation import DataValidator\n",
    "from src.training.config import TrainingConfig\n",
    "from src.training.qlora_trainer import QLoRATrainer\n",
    "from src.evaluation.evaluator import ModelEvaluator\n",
    "from src.evaluation.metrics import MetricsCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6391d3",
   "metadata": {},
   "source": [
    "## 2. Create Sample Dataset\n",
    "\n",
    "For this demo, we'll create a small sample dataset for the healthcare domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69a2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset builder\n",
    "builder = DomainDatasetBuilder(domain=\"healthcare\", output_dir=\"../data/processed\")\n",
    "\n",
    "# Create sample dataset\n",
    "print(\"Creating sample dataset...\")\n",
    "dataset = builder.create_sample_dataset(num_samples=100)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Columns: {dataset.column_names}\")\n",
    "print(\"\\nSample:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e557e",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ccd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(max_length=2048, min_length=10)\n",
    "\n",
    "# Preprocess dataset\n",
    "print(\"Preprocessing dataset...\")\n",
    "text_columns = [\"text\", \"instruction\", \"input\", \"output\"]\n",
    "dataset = preprocessor.apply_preprocessing(dataset, text_columns)\n",
    "dataset = preprocessor.remove_empty_examples(dataset, text_columns)\n",
    "\n",
    "# Compute statistics\n",
    "stats = preprocessor.compute_statistics(dataset, text_column=\"text\")\n",
    "print(\"\\nDataset Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d717ab",
   "metadata": {},
   "source": [
    "## 4. Validate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d89abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validator\n",
    "validator = DataValidator(required_columns=[\"text\"])\n",
    "\n",
    "# Run validation\n",
    "print(\"Validating dataset...\")\n",
    "validation_results = validator.run_full_validation(dataset)\n",
    "\n",
    "# Print report\n",
    "report = validator.generate_validation_report(validation_results)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efd814",
   "metadata": {},
   "source": [
    "## 5. Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c6fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "print(\"Splitting dataset...\")\n",
    "dataset_dict = builder.split_dataset(\n",
    "    dataset,\n",
    "    train_size=0.8,\n",
    "    val_size=0.1,\n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(dataset_dict['train'])} samples\")\n",
    "print(f\"Validation: {len(dataset_dict['validation'])} samples\")\n",
    "print(f\"Test: {len(dataset_dict['test'])} samples\")\n",
    "\n",
    "# Save dataset\n",
    "builder.save_dataset(dataset_dict, \"healthcare_sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef8a6b",
   "metadata": {},
   "source": [
    "## 6. Configure Training\n",
    "\n",
    "For this demo, we'll use a small model and reduced epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bcf6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "config = TrainingConfig(\n",
    "    model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # Small model for demo\n",
    "    output_dir=\"../models/checkpoints/demo\",\n",
    "    num_train_epochs=1,  # Reduced for demo\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=2e-4,\n",
    "    lora_r=8,  # Smaller for faster training\n",
    "    max_seq_length=512,  # Reduced for demo\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Model: {config.model_name}\")\n",
    "print(f\"  Epochs: {config.num_train_epochs}\")\n",
    "print(f\"  Batch size: {config.per_device_train_batch_size}\")\n",
    "print(f\"  LoRA r: {config.lora_r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644acf8f",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "**Note:** This cell requires GPU and may take significant time. Skip if running on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f375378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train (requires GPU)\n",
    "# trainer = QLoRATrainer(config)\n",
    "# metrics = trainer.train(\n",
    "#     train_dataset=dataset_dict['train'],\n",
    "#     eval_dataset=dataset_dict['validation']\n",
    "# )\n",
    "# print(\"Training metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fb4623",
   "metadata": {},
   "source": [
    "## 8. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5443d141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to evaluate (after training)\n",
    "# evaluator = ModelEvaluator(\n",
    "#     model=trainer.model,\n",
    "#     tokenizer=trainer.tokenizer,\n",
    "#     output_dir=\"../results/demo\"\n",
    "# )\n",
    "\n",
    "# eval_metrics = evaluator.evaluate_dataset(\n",
    "#     dataset=dataset_dict['test'],\n",
    "#     input_column=\"input\",\n",
    "#     reference_column=\"output\",\n",
    "#     include_perplexity=False  # Skip for speed\n",
    "# )\n",
    "\n",
    "# print(\"Evaluation Metrics:\")\n",
    "# for key, value in eval_metrics.items():\n",
    "#     print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0b1b0",
   "metadata": {},
   "source": [
    "## 9. Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04bdd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to test generation (after training)\n",
    "# test_prompt = \"Explain what diabetes is.\"\n",
    "# generated = trainer.generate(\n",
    "#     prompt=test_prompt,\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "\n",
    "# print(f\"Prompt: {test_prompt}\")\n",
    "# print(f\"\\nGenerated:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1999d282",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Dataset creation and preprocessing\n",
    "2. ✅ Data validation\n",
    "3. ✅ Dataset splitting\n",
    "4. ⏸️ Model training (requires GPU)\n",
    "5. ⏸️ Model evaluation\n",
    "6. ⏸️ Text generation\n",
    "\n",
    "For full training, use the command-line scripts with GPU resources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
